{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47a0913",
   "metadata": {},
   "source": [
    "#### **Llama3.1B**\n",
    "\n",
    "I downloaded model files from Hugging face : \n",
    "- Link : <link> https://huggingface.co/meta-llama/Llama-3.2-1B/tree/main </link>\n",
    "\n",
    "We will use transformers library to load this model, you can change the filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b0604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6009c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# you can change this path wherever you stored the model files\n",
    "model_path = \"/Users/umesh/Desktop/Llama3.1B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model_causal = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb97a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbd35512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to decode the generated tokens\n",
    "def decode_tokens(generate_ids):\n",
    "    return tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf35aaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generate\n",
    "generate_ids_causal = model_causal.generate(inputs.input_ids, max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0a1da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Response by Causal Model : Hey, are you conscious? Can you talk to me? Hey, '\n",
      " 'are you conscious? Can you talk to me? Hey, are you conscious')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(f\"Response by Causal Model : {decode_tokens(generate_ids_causal)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83adeb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much time it takes for generating response\n",
    "import time \n",
    "import pdb\n",
    "\n",
    "def generate_response(prompt):\n",
    "    start = time.time()\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    # pdb.set_trace()\n",
    "    output_ids = model_causal.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_length=30\n",
    "    )\n",
    "    decoded_output = tokenizer.batch_decode(output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    end = time.time()\n",
    "\n",
    "    total_time = end - start\n",
    "    return decoded_output, total_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c8432f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "output, time = generate_response(\"Hey How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b7937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken : 9.73 min\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time taken : {time/60 :.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e20f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Output : Hey How are you? In this post, we’re going to learn about the best '\n",
      " 'ways to get rid of ants in the house. We’ll')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(f\"Output : {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2949c3",
   "metadata": {},
   "source": [
    "---\n",
    "**I want faster inference**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7772144f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.2',\n",
    "    messages=[{'role':'user','content':'Hello!'}]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a39fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
